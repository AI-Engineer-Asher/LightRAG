{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to build a consultant bot that can answer questions of different domains, such as medical with a doctor bot or legal with a lawyer bot.\n",
    "\n",
    "We will show how flexible ``Component`` and the ``Sequential`` container is to build the same task\n",
    "in different ways.\n",
    "\n",
    "1. **Single Task**: We can build a single task where it deals with multiple generators and handles any coding logic.\n",
    "2. **Multiple Tasks** and combine them using ``Sequential`` which resembles the concept of `Chain` or pipelines in other libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets prepare the imports and prompt templates using `jinjia2` template. We plan to demonstrate how we can use different models too. If this tutorial is the first thing you read, no need to care about more details, but focus on how the `development process` looks like using `LightRAG` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lightrag.core import Component, Generator, Sequential\n",
    "from lightrag.components.model_client import OpenAIClient\n",
    "from lightrag.components.model_client import GroqAPIClient\n",
    "from lightrag.utils import setup_env # make sure you have a .env file with OPENAI_API_KEY and GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_doc = r\"\"\"<SYS> You are a doctor </SYS> User: {{input_str}}\"\"\"\n",
    "template_law = r\"\"\"<SYS> You are a lawyer </SYS> User: {{input_str}}\"\"\"\n",
    "template_router = r\"\"\"<SYS> You are a router who will route a user question to the right generator.\n",
    "            Here are your choices in form of key: value pairs:\n",
    "             {% for key, value in choices.items() %}\n",
    "                {{ key }}: {{ value }}\n",
    "             {% endfor %}\n",
    "            Output the key of your choice.\n",
    "            </SYS> User question: {{input_str}}\n",
    "            You:\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn on the library log to help with debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightrag.utils import enable_library_logging\n",
    "enable_library_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy example\n",
    "\n",
    "class DocQA(Component):\n",
    "    def __init__(self):\n",
    "        super(DocQA, self).__init__()\n",
    "        self.doc = Generator(\n",
    "            template=template_doc,\n",
    "            model_client=OpenAIClient(),\n",
    "            model_kwargs={\"model\": \"gpt-3.5-turbo\"},\n",
    "        )\n",
    "\n",
    "    def call(self, query: str) -> str:\n",
    "        return self.doc(prompt_kwargs={\"input_str\": query}).data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-09 20:25:20 - INFO - [prompt_builder.py:82:__init__] - Prompt has variables: ['input_str']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DocQA(\n",
       "  (doc): Generator(\n",
       "    model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
       "    (prompt): Prompt(template: <SYS> You are a doctor </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
       "    (model_client): OpenAIClient()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = DocQA()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-09 20:25:20 - INFO - [generator.py:196:call] - prompt_kwargs: {'input_str': 'What is the best treatment for headache?'}\n",
      "2024-06-09 20:25:20 - INFO - [generator.py:197:call] - model_kwargs: {}\n",
      "2024-06-09 20:25:20 - INFO - [openai_client.py:122:call] - api_kwargs: {'model': 'gpt-3.5-turbo', 'messages': [{'role': 'system', 'content': '<SYS> You are a doctor </SYS> User: What is the best treatment for headache?'}]}\n",
      "2024-06-09 20:25:22 - INFO - [_client.py:1026:_send_single_request] - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-09 20:25:22 - INFO - [generator.py:205:call] - output: GeneratorOutput(data=\"As a doctor, I would recommend trying over-the-counter pain relievers such as acetaminophen, ibuprofen, or aspirin. It's important to stay well-hydrated, get plenty of rest, and practice relaxation techniques such as deep breathing or meditation. If the headache persists or worsens, it's important to consult with a healthcare provider for further evaluation and treatment options.\", error=None, raw_response=\"As a doctor, I would recommend trying over-the-counter pain relievers such as acetaminophen, ibuprofen, or aspirin. It's important to stay well-hydrated, get plenty of rest, and practice relaxation techniques such as deep breathing or meditation. If the headache persists or worsens, it's important to consult with a healthcare provider for further evaluation and treatment options.\")\n",
      "As a doctor, I would recommend trying over-the-counter pain relievers such as acetaminophen, ibuprofen, or aspirin. It's important to stay well-hydrated, get plenty of rest, and practice relaxation techniques such as deep breathing or meditation. If the headache persists or worsens, it's important to consult with a healthcare provider for further evaluation and treatment options.\n"
     ]
    }
   ],
   "source": [
    "print(doc(\"What is the best treatment for headache?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', DocQA(\n",
      "  (doc): Generator(\n",
      "    model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
      "    (prompt): Prompt(template: <SYS> You are a doctor </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
      "    (model_client): OpenAIClient()\n",
      "  )\n",
      "))\n",
      "('doc', Generator(\n",
      "  model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
      "  (prompt): Prompt(template: <SYS> You are a doctor </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
      "  (model_client): OpenAIClient()\n",
      "))\n",
      "('doc.prompt', Prompt(template: <SYS> You are a doctor </SYS> User: {{input_str}}, prompt_variables: ['input_str']))\n",
      "('doc.model_client', OpenAIClient())\n"
     ]
    }
   ],
   "source": [
    "# list other subcomponents\n",
    "\n",
    "for subcomponent in doc.named_components():\n",
    "    print(subcomponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightrag.core.parameter import Parameter\n",
    "\n",
    "doc.register_parameter(\"demo\", param=Parameter(data=\"demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('demo', Parameter: demo)\n"
     ]
    }
   ],
   "source": [
    "# list all parameters\n",
    "for param in doc.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'DocQA',\n",
       " 'data': {'_components': {'doc': {'type': 'Generator',\n",
       "    'data': {'_components': {'model_client': {'type': 'OpenAIClient',\n",
       "       'data': {'_components': {},\n",
       "        '_parameters': {},\n",
       "        'training': False,\n",
       "        'sync_client': <openai.OpenAI at 0x1191fc250>,\n",
       "        'async_client': None,\n",
       "        '_api_key': None}},\n",
       "      'prompt': {'type': 'Prompt',\n",
       "       'data': {'_components': {},\n",
       "        '_parameters': {},\n",
       "        'training': False,\n",
       "        '_template_string': '<SYS> You are a doctor </SYS> User: {{input_str}}',\n",
       "        'prompt_variables': ['input_str'],\n",
       "        'preset_prompt_kwargs': None}}},\n",
       "     '_parameters': {},\n",
       "     'training': False,\n",
       "     'template': '<SYS> You are a doctor </SYS> User: {{input_str}}',\n",
       "     'preset_prompt_kwargs': None,\n",
       "     'model_kwargs': {'model': 'gpt-3.5-turbo'},\n",
       "     'output_processors': None,\n",
       "     '_trainable_params': []}}},\n",
       "  '_parameters': {'demo': {'data': 'demo', 'requires_opt': True}},\n",
       "  'training': False}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.serialization import save_json\n",
    "\n",
    "save_json(doc.to_dict(), \"doc.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('demo', Parameter: demo)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-09 20:25:22 - INFO - [generator.py:196:call] - prompt_kwargs: {'input_str': 'What is the best treatment for a cold?'}\n",
      "2024-06-09 20:25:22 - INFO - [generator.py:197:call] - model_kwargs: {}\n",
      "2024-06-09 20:25:22 - INFO - [openai_client.py:122:call] - api_kwargs: {'model': 'gpt-3.5-turbo', 'messages': [{'role': 'system', 'content': '<SYS> You are a doctor </SYS> User: What is the best treatment for a cold?'}]}\n",
      "2024-06-09 20:25:25 - INFO - [_client.py:1026:_send_single_request] - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-09 20:25:26 - INFO - [generator.py:205:call] - output: GeneratorOutput(data='As a doctor, I recommend the following treatments for a cold:\\n\\n1. Rest: Get plenty of rest to allow your body to recover and fight off the cold virus.\\n\\n2. Stay hydrated: Drink plenty of fluids such as water, herbal teas, and clear broths to help keep your throat moist and loosen congestion.\\n\\n3. Over-the-counter medications: Consider taking over-the-counter medications such as acetaminophen or ibuprofen to help reduce fever and relieve aches and pains. Decongestants and antihistamines can also help alleviate nasal congestion and runny nose.\\n\\n4. Saline nasal drops or sprays: These can help relieve nasal congestion and improve breathing.\\n\\n5. Gargling with warm salt water: This can help soothe a sore throat.\\n\\n6. Humidifier: Using a humidifier in your room can help keep the air moist and ease congestion.\\n\\n7. Vitamin C and Zinc: Some studies suggest that vitamin C and zinc supplements may help reduce the duration and severity of a cold.\\n\\nRemember to consult with your healthcare provider before starting any new treatment regimen, especially if you have any underlying health conditions or are taking medications.', error=None, raw_response='As a doctor, I recommend the following treatments for a cold:\\n\\n1. Rest: Get plenty of rest to allow your body to recover and fight off the cold virus.\\n\\n2. Stay hydrated: Drink plenty of fluids such as water, herbal teas, and clear broths to help keep your throat moist and loosen congestion.\\n\\n3. Over-the-counter medications: Consider taking over-the-counter medications such as acetaminophen or ibuprofen to help reduce fever and relieve aches and pains. Decongestants and antihistamines can also help alleviate nasal congestion and runny nose.\\n\\n4. Saline nasal drops or sprays: These can help relieve nasal congestion and improve breathing.\\n\\n5. Gargling with warm salt water: This can help soothe a sore throat.\\n\\n6. Humidifier: Using a humidifier in your room can help keep the air moist and ease congestion.\\n\\n7. Vitamin C and Zinc: Some studies suggest that vitamin C and zinc supplements may help reduce the duration and severity of a cold.\\n\\nRemember to consult with your healthcare provider before starting any new treatment regimen, especially if you have any underlying health conditions or are taking medications.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As a doctor, I recommend the following treatments for a cold:\\n\\n1. Rest: Get plenty of rest to allow your body to recover and fight off the cold virus.\\n\\n2. Stay hydrated: Drink plenty of fluids such as water, herbal teas, and clear broths to help keep your throat moist and loosen congestion.\\n\\n3. Over-the-counter medications: Consider taking over-the-counter medications such as acetaminophen or ibuprofen to help reduce fever and relieve aches and pains. Decongestants and antihistamines can also help alleviate nasal congestion and runny nose.\\n\\n4. Saline nasal drops or sprays: These can help relieve nasal congestion and improve breathing.\\n\\n5. Gargling with warm salt water: This can help soothe a sore throat.\\n\\n6. Humidifier: Using a humidifier in your room can help keep the air moist and ease congestion.\\n\\n7. Vitamin C and Zinc: Some studies suggest that vitamin C and zinc supplements may help reduce the duration and severity of a cold.\\n\\nRemember to consult with your healthcare provider before starting any new treatment regimen, especially if you have any underlying health conditions or are taking medications.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.call(\"What is the best treatment for a cold?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "<class 'lightrag.core.component.FunComponent'>\n"
     ]
    }
   ],
   "source": [
    "from lightrag.core.component import FunComponent\n",
    "\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "fun_component = FunComponent(add_one)\n",
    "print(fun_component(1))  \n",
    "print(type(fun_component))  \n",
    "\n",
    "# output:\n",
    "# 2\n",
    "# <class 'core.component.FunComponent'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "<class 'lightrag.core.component.AddOneComponent'>\n"
     ]
    }
   ],
   "source": [
    "from lightrag.core.component import fun_to_component \n",
    "\n",
    "fun_component = fun_to_component(add_one)\n",
    "print(fun_component(1))\n",
    "print(type(fun_component))\n",
    "\n",
    "# output:\n",
    "# 2\n",
    "# <class 'lightrag.core.component.AddOneComponent'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "<class 'lightrag.core.component.AddOneComponent'>\n"
     ]
    }
   ],
   "source": [
    "# use it as a decorator\n",
    "@fun_to_component\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "print(add_one(1))\n",
    "print(type(add_one))\n",
    "\n",
    "# output:\n",
    "# 2\n",
    "# <class 'lightrag.core.component.AddOneComponent'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-09 20:25:26 - INFO - [generator.py:196:call] - prompt_kwargs: {'input_str': 'What is the best treatment for headache?Please be concise and only list the top treatments.'}\n",
      "2024-06-09 20:25:26 - INFO - [generator.py:197:call] - model_kwargs: {}\n",
      "2024-06-09 20:25:26 - INFO - [openai_client.py:122:call] - api_kwargs: {'model': 'gpt-3.5-turbo', 'messages': [{'role': 'system', 'content': '<SYS> You are a doctor </SYS> User: What is the best treatment for headache?Please be concise and only list the top treatments.'}]}\n",
      "2024-06-09 20:25:26 - INFO - [_client.py:1026:_send_single_request] - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-09 20:25:26 - INFO - [generator.py:205:call] - output: GeneratorOutput(data='1. Over-the-counter pain medications such as ibuprofen or acetaminophen\\n2. Getting enough rest and staying hydrated\\n3. Using cold or warm compress on the forehead\\n4. Practicing relaxation techniques like deep breathing or meditation', error=None, raw_response='1. Over-the-counter pain medications such as ibuprofen or acetaminophen\\n2. Getting enough rest and staying hydrated\\n3. Using cold or warm compress on the forehead\\n4. Practicing relaxation techniques like deep breathing or meditation')\n",
      "1. Over-the-counter pain medications such as ibuprofen or acetaminophen\n",
      "2. Getting enough rest and staying hydrated\n",
      "3. Using cold or warm compress on the forehead\n",
      "4. Practicing relaxation techniques like deep breathing or meditation\n"
     ]
    }
   ],
   "source": [
    "from lightrag.core.component import Sequential\n",
    "\n",
    "@fun_to_component\n",
    "def enhance_query(query:str) -> str:\n",
    "    return query + \"Please be concise and only list the top treatments.\"\n",
    "\n",
    "seq = Sequential(enhance_query, doc)\n",
    "\n",
    "query = \"What is the best treatment for headache?\"\n",
    "print(seq(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): EnhanceQueryComponent()\n",
       "  (1): DocQA(\n",
       "    (doc): Generator(\n",
       "      model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
       "      (prompt): Prompt(template: <SYS> You are a doctor </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
       "      (model_client): OpenAIClient()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our first approach to build a single task with multiple generators and call each conditionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChatBotWithRouter(Component):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model_1_kwargs = {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "        }\n",
    "        model_2_kwargs = {\"model\": \"llama3-8b-8192\"}\n",
    "        self.doc = Generator(\n",
    "            template=template_doc,\n",
    "            model_client=OpenAIClient(),\n",
    "            model_kwargs=model_1_kwargs,\n",
    "        )\n",
    "        self.lawyer = Generator(\n",
    "            template=template_law,\n",
    "            model_client=GroqAPIClient(),\n",
    "            model_kwargs=model_2_kwargs,\n",
    "        )\n",
    "        self.router_choices = {\n",
    "            \"doctor\": self.create_generator_signature(self.doc),\n",
    "            \"lawyer\": self.create_generator_signature(self.lawyer),\n",
    "            \"other\": \"Choose me the question does not apply to other choices.\",\n",
    "        }\n",
    "        print(self.router_choices)\n",
    "\n",
    "        self.router = Generator(\n",
    "            template=template_router,\n",
    "            model_client=OpenAIClient(),\n",
    "            model_kwargs=model_1_kwargs,\n",
    "        )\n",
    "\n",
    "    def call(self, query: str) -> str:\n",
    "        choice = self.router(\n",
    "            prompt_kwargs={\"input_str\": query, \"choices\": self.router_choices}\n",
    "        ).data\n",
    "        if choice == \"doctor\":\n",
    "            return self.doc(prompt_kwargs={\"input_str\": query}).data\n",
    "        elif choice == \"lawyer\":\n",
    "            return self.lawyer(prompt_kwargs={\"input_str\": query}).data\n",
    "        else:\n",
    "            return \"Sorry, I cannot help you with that.\"\n",
    "\n",
    "    def create_generator_signature(self, generator: Generator):\n",
    "        template = generator.template\n",
    "        pattern = r\"<SYS>(.*?)</SYS>\"\n",
    "\n",
    "        matches = re.findall(pattern, template)\n",
    "        for match in matches:\n",
    "            print(\"Content between <SYS> tags:\", match)\n",
    "            return match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate the task component, and print the task details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-09 20:25:26 - INFO - [prompt_builder.py:82:__init__] - Prompt has variables: ['input_str']\n",
      "2024-06-09 20:25:26 - INFO - [prompt_builder.py:82:__init__] - Prompt has variables: ['input_str']\n",
      "Content between <SYS> tags:  You are a doctor \n",
      "Content between <SYS> tags:  You are a lawyer \n",
      "{'doctor': ' You are a doctor ', 'lawyer': ' You are a lawyer ', 'other': 'Choose me the question does not apply to other choices.'}\n",
      "2024-06-09 20:25:26 - INFO - [prompt_builder.py:82:__init__] - Prompt has variables: ['input_str', 'choices']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatBotWithRouter(\n",
       "  (doc): Generator(\n",
       "    model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
       "    (prompt): Prompt(template: <SYS> You are a doctor </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
       "    (model_client): OpenAIClient()\n",
       "  )\n",
       "  (lawyer): Generator(\n",
       "    model_kwargs={'model': 'llama3-8b-8192'}, model_type=ModelType.LLM\n",
       "    (prompt): Prompt(template: <SYS> You are a lawyer </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
       "    (model_client): GroqAPIClient()\n",
       "  )\n",
       "  (router): Generator(\n",
       "    model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
       "    (prompt): Prompt(\n",
       "      template: <SYS> You are a router who will route a user question to the right generator.\n",
       "                  Here are your choices in form of key: value pairs:\n",
       "                   {% for key, value in choices.items() %}\n",
       "                      {{ key }}: {{ value }}\n",
       "                   {% endfor %}\n",
       "                  Output the key of your choice.\n",
       "                  </SYS> User question: {{input_str}}\n",
       "                  You:\n",
       "                  , prompt_variables: ['input_str', 'choices']\n",
       "    )\n",
       "    (model_client): OpenAIClient()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = ChatBotWithRouter()\n",
    "task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the task with a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-09 20:25:26 - INFO - [generator.py:196:call] - prompt_kwargs: {'input_str': 'I have a legal question', 'choices': {'doctor': ' You are a doctor ', 'lawyer': ' You are a lawyer ', 'other': 'Choose me the question does not apply to other choices.'}}\n",
      "2024-06-09 20:25:26 - INFO - [generator.py:197:call] - model_kwargs: {}\n",
      "2024-06-09 20:25:26 - INFO - [openai_client.py:122:call] - api_kwargs: {'model': 'gpt-3.5-turbo', 'messages': [{'role': 'system', 'content': '<SYS> You are a router who will route a user question to the right generator.\\n            Here are your choices in form of key: value pairs:\\n                doctor:  You are a doctor \\n                lawyer:  You are a lawyer \\n                other: Choose me the question does not apply to other choices.\\n            Output the key of your choice.\\n            </SYS> User question: I have a legal question\\n            You:'}]}\n",
      "2024-06-09 20:25:27 - INFO - [_client.py:1026:_send_single_request] - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-09 20:25:27 - INFO - [generator.py:205:call] - output: GeneratorOutput(data='lawyer', error=None, raw_response='lawyer')\n",
      "2024-06-09 20:25:27 - INFO - [generator.py:196:call] - prompt_kwargs: {'input_str': 'I have a legal question'}\n",
      "2024-06-09 20:25:27 - INFO - [generator.py:197:call] - model_kwargs: {}\n",
      "2024-06-09 20:25:27 - INFO - [_client.py:1026:_send_single_request] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-09 20:25:27 - INFO - [generator.py:205:call] - output: GeneratorOutput(data=\"I'd be happy to help you with your legal question. Please provide more details about the issue you're concerned about, such as the facts of the situation, the laws that are relevant to the issue, and any relevant documents or agreements. The more information you provide, the better I'll be able to understand your situation and provide a thoughtful and accurate response.\", error=None, raw_response=\"I'd be happy to help you with your legal question. Please provide more details about the issue you're concerned about, such as the facts of the situation, the laws that are relevant to the issue, and any relevant documents or agreements. The more information you provide, the better I'll be able to understand your situation and provide a thoughtful and accurate response.\")\n",
      "I'd be happy to help you with your legal question. Please provide more details about the issue you're concerned about, such as the facts of the situation, the laws that are relevant to the issue, and any relevant documents or agreements. The more information you provide, the better I'll be able to understand your situation and provide a thoughtful and accurate response.\n"
     ]
    }
   ],
   "source": [
    "query = \"I have a legal question\"\n",
    "print(task(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's separate this into multiple subtasks and ``chain`` them using the ``Sequential`` container.\n",
    "\n",
    "First, the router task which will takes a dictionary of choices and return the selected key. In addition, we use ``_extra_repr`` to improve the default string representation of the task.\n",
    "\n",
    "As ``Sequential`` will pass the output of one task to the next using positional arguments, we return whatever is needed to the next task in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "class Router(Component):\n",
    "    def __init__(self, choices: Dict[str, str] = {}):\n",
    "        super().__init__()\n",
    "        self.choices = choices\n",
    "        self.router = Generator(\n",
    "            template=template_router,\n",
    "            model_client=OpenAIClient(),\n",
    "            model_kwargs={\"model\": \"gpt-3.5-turbo\"},\n",
    "        )\n",
    "\n",
    "    def call(self, query: str) -> str:\n",
    "        prompt_kwargs = {\"input_str\": query, \"choices\": self.choices}\n",
    "        choice =  self.router(prompt_kwargs=prompt_kwargs).data\n",
    "        return {\"choice\": choice, \"query\": query}\n",
    "    \n",
    "    def _extra_repr(self):\n",
    "        return f\"Choices: {self.choices}, \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the structure of router task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-09 20:25:27 - INFO - [prompt_builder.py:82:__init__] - Prompt has variables: ['input_str', 'choices']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Router(\n",
       "  Choices: {}, \n",
       "  (router): Generator(\n",
       "    model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
       "    (prompt): Prompt(\n",
       "      template: <SYS> You are a router who will route a user question to the right generator.\n",
       "                  Here are your choices in form of key: value pairs:\n",
       "                   {% for key, value in choices.items() %}\n",
       "                      {{ key }}: {{ value }}\n",
       "                   {% endfor %}\n",
       "                  Output the key of your choice.\n",
       "                  </SYS> User question: {{input_str}}\n",
       "                  You:\n",
       "                  , prompt_variables: ['input_str', 'choices']\n",
       "    )\n",
       "    (model_client): OpenAIClient()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Router()\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets build another subtask which handles the chat depending on the selected key from the router task.\n",
    "As the router task returns a dictionary, we will make our input dictionary type that parses the ``choice`` and ``query`` key value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat(Component):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.doc = Generator(\n",
    "            template=template_doc,\n",
    "            model_client=OpenAIClient(),\n",
    "            model_kwargs={\"model\": \"gpt-3.5-turbo\"},\n",
    "        )\n",
    "        self.lawyer = Generator(\n",
    "            template=template_law,\n",
    "            model_client=GroqAPIClient(),\n",
    "            model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
    "        )\n",
    "    # to chain together just to make sure the output can be directly passed to the next as input\n",
    "    def call(self, input: Dict[str, str]) -> Dict[str, str]:\n",
    "        choice = input.get(\"choice\", None)\n",
    "        query = input.get(\"query\", None)\n",
    "        if choice == \"doctor\":\n",
    "            return self.doc(prompt_kwargs={\"input_str\": query}).data\n",
    "        elif choice == \"lawyer\":\n",
    "            return self.lawyer(prompt_kwargs={\"input_str\": query}).data\n",
    "        else:\n",
    "            return \"Sorry, I am not able to help you with that.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-09 20:25:27 - INFO - [prompt_builder.py:82:__init__] - Prompt has variables: ['input_str']\n",
      "2024-06-09 20:25:27 - INFO - [prompt_builder.py:82:__init__] - Prompt has variables: ['input_str']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chat(\n",
       "  (doc): Generator(\n",
       "    model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
       "    (prompt): Prompt(template: <SYS> You are a doctor </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
       "    (model_client): OpenAIClient()\n",
       "  )\n",
       "  (lawyer): Generator(\n",
       "    model_kwargs={'model': 'llama3-8b-8192'}, model_type=ModelType.LLM\n",
       "    (prompt): Prompt(template: <SYS> You are a lawyer </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
       "    (model_client): GroqAPIClient()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat()\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets chain the router and the chat task using the ``Sequential`` container into a runnable pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAWithRouter(Component):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.router = Router(choices={\"doctor\": \"Doctor\", \"lawyer\": \"Lawyer\", \"other\": \"Other\"})\n",
    "        self.chat = Chat()\n",
    "        self.pipeline = Sequential(self.router, self.chat)\n",
    "\n",
    "    def call(self, query: str) -> str:\n",
    "        return self.pipeline(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-09 20:25:27 - INFO - [prompt_builder.py:82:__init__] - Prompt has variables: ['input_str', 'choices']\n",
      "2024-06-09 20:25:27 - INFO - [prompt_builder.py:82:__init__] - Prompt has variables: ['input_str']\n",
      "2024-06-09 20:25:27 - INFO - [prompt_builder.py:82:__init__] - Prompt has variables: ['input_str']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QAWithRouter(\n",
       "  (router): Router(\n",
       "    Choices: {'doctor': 'Doctor', 'lawyer': 'Lawyer', 'other': 'Other'}, \n",
       "    (router): Generator(\n",
       "      model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
       "      (prompt): Prompt(\n",
       "        template: <SYS> You are a router who will route a user question to the right generator.\n",
       "                    Here are your choices in form of key: value pairs:\n",
       "                     {% for key, value in choices.items() %}\n",
       "                        {{ key }}: {{ value }}\n",
       "                     {% endfor %}\n",
       "                    Output the key of your choice.\n",
       "                    </SYS> User question: {{input_str}}\n",
       "                    You:\n",
       "                    , prompt_variables: ['input_str', 'choices']\n",
       "      )\n",
       "      (model_client): OpenAIClient()\n",
       "    )\n",
       "  )\n",
       "  (chat): Chat(\n",
       "    (doc): Generator(\n",
       "      model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
       "      (prompt): Prompt(template: <SYS> You are a doctor </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
       "      (model_client): OpenAIClient()\n",
       "    )\n",
       "    (lawyer): Generator(\n",
       "      model_kwargs={'model': 'llama3-8b-8192'}, model_type=ModelType.LLM\n",
       "      (prompt): Prompt(template: <SYS> You are a lawyer </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
       "      (model_client): GroqAPIClient()\n",
       "    )\n",
       "  )\n",
       "  (pipeline): Sequential(\n",
       "    (0): Router(\n",
       "      Choices: {'doctor': 'Doctor', 'lawyer': 'Lawyer', 'other': 'Other'}, \n",
       "      (router): Generator(\n",
       "        model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
       "        (prompt): Prompt(\n",
       "          template: <SYS> You are a router who will route a user question to the right generator.\n",
       "                      Here are your choices in form of key: value pairs:\n",
       "                       {% for key, value in choices.items() %}\n",
       "                          {{ key }}: {{ value }}\n",
       "                       {% endfor %}\n",
       "                      Output the key of your choice.\n",
       "                      </SYS> User question: {{input_str}}\n",
       "                      You:\n",
       "                      , prompt_variables: ['input_str', 'choices']\n",
       "        )\n",
       "        (model_client): OpenAIClient()\n",
       "      )\n",
       "    )\n",
       "    (1): Chat(\n",
       "      (doc): Generator(\n",
       "        model_kwargs={'model': 'gpt-3.5-turbo'}, model_type=ModelType.LLM\n",
       "        (prompt): Prompt(template: <SYS> You are a doctor </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
       "        (model_client): OpenAIClient()\n",
       "      )\n",
       "      (lawyer): Generator(\n",
       "        model_kwargs={'model': 'llama3-8b-8192'}, model_type=ModelType.LLM\n",
       "        (prompt): Prompt(template: <SYS> You are a lawyer </SYS> User: {{input_str}}, prompt_variables: ['input_str'])\n",
       "        (model_client): GroqAPIClient()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = QAWithRouter()\n",
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-09 20:25:27 - INFO - [generator.py:196:call] - prompt_kwargs: {'input_str': 'I have a legal question', 'choices': {'doctor': 'Doctor', 'lawyer': 'Lawyer', 'other': 'Other'}}\n",
      "2024-06-09 20:25:27 - INFO - [generator.py:197:call] - model_kwargs: {}\n",
      "2024-06-09 20:25:27 - INFO - [openai_client.py:122:call] - api_kwargs: {'model': 'gpt-3.5-turbo', 'messages': [{'role': 'system', 'content': '<SYS> You are a router who will route a user question to the right generator.\\n            Here are your choices in form of key: value pairs:\\n                doctor: Doctor\\n                lawyer: Lawyer\\n                other: Other\\n            Output the key of your choice.\\n            </SYS> User question: I have a legal question\\n            You:'}]}\n",
      "2024-06-09 20:25:28 - INFO - [_client.py:1026:_send_single_request] - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-09 20:25:28 - INFO - [generator.py:205:call] - output: GeneratorOutput(data='other', error=None, raw_response='other')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sorry, I am not able to help you with that.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"I have a legal question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a router and two generators\n",
    "# every single component might need a signature.\n",
    "# TODO: LLM for single choices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-project-kernel",
   "language": "python",
   "name": "my-project-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
